{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI Document Assistant - RAG System with PDF and Confluence Integration\n\nThis notebook contains a complete RAG system with Streamlit frontend for Kaggle deployment.\n\n## 🚀 Installation and Setup","metadata":{}},{"cell_type":"code","source":"\n# Install required packages\n!pip install -q streamlit langchain langchain-community langchain-huggingface\n!pip install -q faiss-cpu sentence-transformers PyMuPDF atlassian-python-api\n!pip install -q boto3 langchain-aws python-dotenv jinja2 beautifulsoup4\n!pip install -q torch torchvision\n!pip install -q pyngrok openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:20:02.751812Z","iopub.execute_input":"2025-07-27T05:20:02.752033Z","iopub.status.idle":"2025-07-27T05:22:06.868189Z","shell.execute_reply.started":"2025-07-27T05:20:02.752013Z","shell.execute_reply":"2025-07-27T05:22:06.866702Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.5/193.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import libraries and setup\nimport os\nimport sys\nimport tempfile\nimport subprocess\nimport traceback\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create working directory\nwork_dir = Path('/kaggle/working')\nwork_dir.mkdir(exist_ok=True)\nos.chdir(work_dir)\n\nprint('✅ Setup complete!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:22:14.384237Z","iopub.execute_input":"2025-07-27T05:22:14.384552Z","iopub.status.idle":"2025-07-27T05:22:14.392314Z","shell.execute_reply.started":"2025-07-27T05:22:14.384517Z","shell.execute_reply":"2025-07-27T05:22:14.391316Z"}},"outputs":[{"name":"stdout","text":"✅ Setup complete!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 📁 Core Components Implementation\n\n### 1. Confluence Integration Module","metadata":{}},{"cell_type":"code","source":"%%writefile extract_confluence.py\nfrom atlassian import Confluence\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.schema import Document\nimport os\nimport traceback\nfrom dotenv import load_dotenv\nfrom bs4 import BeautifulSoup\n\nload_dotenv()\n\nclass ConfluenceProcessor:\n    def __init__(self):\n        self.confluence = None\n        self.available = False\n        self.error_message = None\n        \n        # FIXED: Proper validation of Confluence credentials\n        confluence_url = os.getenv('CONFLUENCE_URL', '').strip()\n        confluence_username = os.getenv('CONFLUENCE_USERNAME', '').strip()\n        confluence_token = os.getenv('CONFLUENCE_API_TOKEN', '').strip()\n        \n        if not confluence_url or not confluence_username or not confluence_token:\n            self.error_message = 'Missing Confluence credentials (URL, USERNAME, or API_TOKEN)'\n            print(f'Confluence not configured: {self.error_message}')\n            return\n        \n        try:\n            self.confluence = Confluence(\n                url=confluence_url,\n                username=confluence_username,\n                password=confluence_token\n            )\n            \n            # Test actual connection\n            test_result = self.confluence.get_all_spaces(start=0, limit=1)\n            if test_result:\n                self.available = True\n                self.embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n                print('Confluence connection verified')\n            else:\n                self.error_message = 'Unable to fetch spaces - check permissions'\n                print(f'Confluence test failed: {self.error_message}')\n        except Exception as e:\n            self.error_message = f'Connection failed: {str(e)}'\n            print(f'Confluence connection error: {self.error_message}')\n    \n    def get_connection_status(self):\n        return {\n            'available': self.available,\n            'error': self.error_message,\n            'url': os.getenv('CONFLUENCE_URL', 'Not set'),\n            'username': os.getenv('CONFLUENCE_USERNAME', 'Not set')\n        }\n    \n    def clean_html_content(self, html_content):\n        if not html_content:\n            return ''\n        try:\n            soup = BeautifulSoup(html_content, 'html.parser')\n            for script in soup(['script', 'style']):\n                script.decompose()\n            text = soup.get_text()\n            lines = (line.strip() for line in text.splitlines())\n            chunks = (phrase.strip() for line in lines for phrase in line.split('  '))\n            text = ' '.join(chunk for chunk in chunks if chunk)\n            return text\n        except Exception as e:\n            print(f'Error cleaning HTML: {e}')\n            return str(html_content)[:1000]\n    \n    def get_page_content(self, page_id):\n        if not self.available:\n            return None\n        try:\n            page = self.confluence.get_page_by_id(page_id, expand='body.storage,version,space')\n            title = page['title']\n            content = page['body']['storage']['value']\n            space_key = page['space']['key']\n            clean_content = self.clean_html_content(content)\n            return {\n                'title': title,\n                'content': clean_content,\n                'page_id': page_id,\n                'space_key': space_key,\n                'url': f\"{os.getenv('CONFLUENCE_URL')}/pages/viewpage.action?pageId={page_id}\"\n            }\n        except Exception as e:\n            print(f'Error fetching page {page_id}: {str(e)}')\n            return None\n    \n    def process_confluence_to_vectorstore(self, page_ids=None, chunk_size=500):\n        if not self.available:\n            print(f'Confluence not available: {self.error_message}')\n            return None\n        \n        documents = []\n        if page_ids:\n            for page_id in page_ids:\n                page_data = self.get_page_content(page_id)\n                if page_data:\n                    doc = Document(\n                        page_content=f\"Title: {page_data['title']}\\n\\n{page_data['content']}\",\n                        metadata={\n                            'source': 'confluence',\n                            'title': page_data['title'],\n                            'page_id': page_data['page_id'],\n                            'space_key': page_data['space_key'],\n                            'url': page_data['url']\n                        }\n                    )\n                    documents.append(doc)\n        \n        if not documents:\n            print('No documents found to process')\n            return None\n        \n        print(f'Loaded {len(documents)} documents from Confluence')\n        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=100)\n        chunks = splitter.split_documents(documents)\n        print(f'Split into {len(chunks)} chunks.')\n        vectorstore = FAISS.from_documents(chunks, self.embeddings)\n        print('Stored Confluence chunks in FAISS vector store.')\n        return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:22:19.866242Z","iopub.execute_input":"2025-07-27T05:22:19.866523Z","iopub.status.idle":"2025-07-27T05:22:19.877287Z","shell.execute_reply.started":"2025-07-27T05:22:19.866503Z","shell.execute_reply":"2025-07-27T05:22:19.876320Z"}},"outputs":[{"name":"stdout","text":"Writing extract_confluence.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### 2. Unified Data Processor","metadata":{}},{"cell_type":"code","source":"%%writefile unified_processor.py\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom extract_confluence import ConfluenceProcessor\nimport os\nimport traceback\nimport sys\n\nclass UnifiedDataProcessor:\n    def __init__(self, vector_store_path='./vector_store/'):\n        self.vector_store_path = vector_store_path\n        self.embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n        self.confluence_processor = ConfluenceProcessor()\n        self.vectorstore = None\n        os.makedirs(vector_store_path, exist_ok=True)\n        self.load_existing_vectorstore()\n    \n    def load_existing_vectorstore(self):\n        try:\n            if os.path.exists(os.path.join(self.vector_store_path, 'index.faiss')):\n                self.vectorstore = FAISS.load_local(\n                    self.vector_store_path,\n                    embeddings=self.embeddings,\n                    allow_dangerous_deserialization=True\n                )\n                print('Loaded existing vector store')\n            else:\n                print('No existing vector store found')\n        except Exception as e:\n            print(f'Error loading existing vector store: {str(e)}')\n            traceback.print_exc()\n    \n    def add_pdf_documents(self, pdf_paths, chunk_size=500):\n        all_documents = []\n        if isinstance(pdf_paths, str):\n            pdf_paths = [pdf_paths]\n        \n        for pdf_path in pdf_paths:\n            try:\n                loader = PyPDFLoader(pdf_path)\n                documents = loader.load()\n                \n                # FIXED: Enhanced metadata for better search results\n                for i, doc in enumerate(documents):\n                    doc.metadata['source'] = 'pdf'\n                    doc.metadata['file_path'] = pdf_path\n                    doc.metadata['file_name'] = os.path.basename(pdf_path)\n                    doc.metadata['title'] = os.path.splitext(os.path.basename(pdf_path))[0]\n                    doc.metadata['page'] = i + 1\n                    doc.metadata['total_pages'] = len(documents)\n                    \n                    # Ensure content is meaningful\n                    if not doc.page_content or len(doc.page_content.strip()) < 10:\n                        doc.page_content = f'Content from {doc.metadata[\"title\"]} - Page {doc.metadata[\"page\"]}'\n                \n                all_documents.extend(documents)\n                print(f'Loaded {len(documents)} pages from {os.path.basename(pdf_path)}')\n            except Exception as e:\n                print(f'Error processing PDF {pdf_path}: {str(e)}')\n                traceback.print_exc()\n        \n        if all_documents:\n            self._add_documents_to_vectorstore(all_documents, chunk_size)\n    \n    def add_confluence_documents(self, page_ids=None, chunk_size=500):\n        try:\n            confluence_vectorstore = self.confluence_processor.process_confluence_to_vectorstore(\n                page_ids=page_ids, chunk_size=chunk_size\n            )\n            if confluence_vectorstore:\n                if self.vectorstore is None:\n                    self.vectorstore = confluence_vectorstore\n                else:\n                    self.vectorstore.merge_from(confluence_vectorstore)\n                self.save_vectorstore()\n                print('Successfully added Confluence documents to vector store')\n            else:\n                print('Failed to process Confluence documents')\n        except Exception as e:\n            print(f'Error adding Confluence documents: {str(e)}')\n            traceback.print_exc()\n    \n    def _add_documents_to_vectorstore(self, documents, chunk_size):\n        try:\n            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=100)\n            chunks = splitter.split_documents(documents)\n            print(f'Split into {len(chunks)} chunks.')\n            \n            if self.vectorstore is None:\n                self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n            else:\n                new_vectorstore = FAISS.from_documents(chunks, self.embeddings)\n                self.vectorstore.merge_from(new_vectorstore)\n            \n            self.save_vectorstore()\n            print('Added documents to unified vector store.')\n        except Exception as e:\n            print(f'Error adding documents to vector store: {str(e)}')\n            traceback.print_exc()\n    \n    def save_vectorstore(self):\n        try:\n            if self.vectorstore:\n                self.vectorstore.save_local(self.vector_store_path)\n                print(f'Vector store saved to {self.vector_store_path}')\n        except Exception as e:\n            print(f'Error saving vector store: {str(e)}')\n            traceback.print_exc()\n    \n    def get_vectorstore(self):\n        return self.vectorstore\n    \n    # FIXED: Enhanced search with proper error handling and source information\n    def search_documents(self, query, k=5):\n        if self.vectorstore is None:\n            print('No vector store available')\n            return []\n        \n        if not query or not query.strip():\n            print('Empty query provided')\n            return []\n        \n        try:\n            print(f'Searching for: \"{query}\" (k={k})')\n            results = self.vectorstore.similarity_search_with_score(query, k=k)\n            print(f'Found {len(results)} results')\n            \n            # Enhanced result formatting with source information\n            formatted_results = []\n            for doc, score in results:\n                # Ensure all metadata fields exist\n                metadata = doc.metadata.copy()\n                metadata.setdefault('title', 'Unknown Document')\n                metadata.setdefault('source', 'unknown')\n                metadata.setdefault('file_name', 'Unknown File')\n                \n                formatted_results.append((doc, score))\n            \n            return formatted_results\n        except Exception as e:\n            print(f'Error searching documents: {str(e)}')\n            print(f'Vector store type: {type(self.vectorstore)}')\n            print(f'Query type: {type(query)}, length: {len(query) if query else 0}')\n            traceback.print_exc()\n            return []\n    \n    def get_confluence_status(self):\n        return self.confluence_processor.get_connection_status()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:22:31.502498Z","iopub.execute_input":"2025-07-27T05:22:31.502934Z","iopub.status.idle":"2025-07-27T05:22:31.512531Z","shell.execute_reply.started":"2025-07-27T05:22:31.502904Z","shell.execute_reply":"2025-07-27T05:22:31.511569Z"}},"outputs":[{"name":"stdout","text":"Writing unified_processor.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### 3. Design Document Generator","metadata":{}},{"cell_type":"code","source":"%%writefile design_doc_generator.py\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.prompts import PromptTemplate\nfrom jinja2 import Template\nimport os\nimport traceback\nfrom datetime import datetime\nimport re\nfrom collections import defaultdict\n\nclass DesignDocumentGenerator:\n    def __init__(self, vector_store_path='./vector_store/'):\n        self.vector_store_path = vector_store_path\n        self._embeddings = None\n        self._processor = None\n        self._vectorstore = None\n        \n        # Enhanced template with better structure and RAG integration\n        self.design_doc_template = '''# {{title}}\n\n**Document Type:** Technical Design Document\n**Generated:** {{timestamp}}\n**Version:** 1.0\n**Status:** Draft\n\n---\n\n## 1. Executive Summary\n{{overview}}\n\n## 2. Background and Context\n{{background}}\n\n## 3. Requirements Analysis\n{{requirements}}\n\n## 4. System Architecture\n{{architecture}}\n\n## 5. Technical Implementation\n{{implementation}}\n\n## 6. Data Flow and Integration\n{{data_flow}}\n\n## 7. Security Considerations\n{{security}}\n\n## 8. Performance and Scalability\n{{performance}}\n\n## 9. Testing Strategy\n{{testing}}\n\n## 10. Deployment Plan\n{{deployment}}\n\n## 11. Timeline and Milestones\n{{timeline}}\n\n## 12. Risk Assessment\n{{risks}}\n\n## 13. References and Sources\n{{references}}\n\n---\n**Document Metadata:**\n- Sources analyzed: {{source_count}} documents\n- Content types: {{content_types}}\n- Generated using RAG-enhanced analysis\n- Last updated: {{timestamp}}\n'''\n\n    @property\n    def embeddings(self):\n        if self._embeddings is None:\n            self._embeddings = HuggingFaceEmbeddings(\n                model_name='sentence-transformers/all-MiniLM-L6-v2',\n                model_kwargs={'device': 'cpu'},\n                encode_kwargs={'normalize_embeddings': True}\n            )\n        return self._embeddings\n\n    @property\n    def processor(self):\n        if self._processor is None:\n            try:\n                from unified_processor_fast import UnifiedDataProcessor\n            except ImportError:\n                try:\n                    from unified_processor import UnifiedDataProcessor\n                except ImportError:\n                    from unified_processor_fixed import UnifiedDataProcessor\n            self._processor = UnifiedDataProcessor(self.vector_store_path)\n        return self._processor\n\n    @property\n    def vectorstore(self):\n        if self._vectorstore is None:\n            self._vectorstore = self.processor.get_vectorstore()\n        return self._vectorstore\n\n    def _extract_relevant_content(self, user_request, k=10):\n        \"\"\"Extract and analyze relevant content from documents using RAG\"\"\"\n        if not self.vectorstore:\n            return [], {}, \"\"\n        \n        try:\n            print(f\"🔍 Searching for relevant content: {user_request}\")\n            # Search for relevant documents\n            results = self.processor.search_documents(user_request, k=k)\n            \n            sources = []\n            content_by_type = defaultdict(list)\n            all_content = \"\"\n            \n            for doc, score in results:\n                # Extract comprehensive metadata\n                source_info = {\n                    'title': doc.metadata.get('title', doc.metadata.get('file_name', 'Unknown')),\n                    'type': doc.metadata.get('source', 'unknown'),\n                    'score': float(score),\n                    'url': doc.metadata.get('url', ''),\n                    'page': doc.metadata.get('page', ''),\n                    'content_preview': doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n                }\n                sources.append(source_info)\n                \n                # Group content by type for better analysis\n                content_type = doc.metadata.get('source', 'unknown')\n                content_by_type[content_type].append({\n                    'title': source_info['title'],\n                    'content': doc.page_content,\n                    'score': score\n                })\n                \n                # Accumulate all content for comprehensive analysis\n                all_content += f\"\\n\\n--- From {source_info['title']} ---\\n{doc.page_content}\"\n            \n            print(f\"📚 Found {len(sources)} relevant sources across {len(content_by_type)} content types\")\n            return sources, dict(content_by_type), all_content\n            \n        except Exception as e:\n            print(f\"❌ Content extraction error: {e}\")\n            return [], {}, \"\"\n\n    def _extract_key_terms(self, content, user_request):\n        \"\"\"Extract key technical terms and concepts from retrieved content\"\"\"\n        technical_terms = []\n        \n        # Common technical patterns to look for in the content\n        patterns = [\n            r'\\b(API|REST|GraphQL|microservice|database|authentication|authorization)\\b',\n            r'\\b(Docker|Kubernetes|AWS|Azure|GCP|cloud)\\b',\n            r'\\b(React|Angular|Vue|Node\\.js|Python|Java|Go|JavaScript)\\b',\n            r'\\b(PostgreSQL|MySQL|MongoDB|Redis|SQL)\\b',\n            r'\\b(OAuth|JWT|SSL|TLS|HTTPS|security)\\b',\n            r'\\b(CI/CD|DevOps|deployment|monitoring)\\b'\n        ]\n        \n        for pattern in patterns:\n            matches = re.findall(pattern, content, re.IGNORECASE)\n            technical_terms.extend(matches)\n        \n        # Remove duplicates and return top terms\n        return list(set(technical_terms))[:10]\n\n    def _extract_requirements_from_content(self, content):\n        \"\"\"Extract functional and non-functional requirements from content\"\"\"\n        requirements = {\n            'functional': [],\n            'non_functional': []\n        }\n        \n        # Look for requirement patterns in the content\n        func_patterns = [\n            r'must\\s+(be able to|support|provide|allow)\\s+([^.]+)',\n            r'shall\\s+([^.]+)',\n            r'requirement[s]?[:]?\\s+([^.]+)',\n            r'should\\s+(be able to|support|provide|allow)\\s+([^.]+)'\n        ]\n        \n        for pattern in func_patterns:\n            matches = re.findall(pattern, content, re.IGNORECASE)\n            for match in matches:\n                req = match if isinstance(match, str) else match[-1]\n                if len(req.strip()) > 10:  # Only meaningful requirements\n                    requirements['functional'].append(req.strip())\n        \n        # Non-functional requirements\n        nf_patterns = [\n            r'performance[:]?\\s+([^.]+)',\n            r'scalability[:]?\\s+([^.]+)',\n            r'security[:]?\\s+([^.]+)',\n            r'availability[:]?\\s+([^.]+)',\n            r'response time[:]?\\s+([^.]+)'\n        ]\n        \n        for pattern in nf_patterns:\n            matches = re.findall(pattern, content, re.IGNORECASE)\n            requirements['non_functional'].extend([match.strip() for match in matches if len(match.strip()) > 5])\n        \n        return requirements\n\n    def _analyze_content_for_section(self, section_type, user_request, content_by_type, all_content):\n        \"\"\"Analyze retrieved content to generate contextual section content\"\"\"\n        \n        # Extract insights from the retrieved content\n        key_terms = self._extract_key_terms(all_content, user_request)\n        requirements = self._extract_requirements_from_content(all_content)\n        \n        if section_type == 'overview':\n            return self._generate_contextual_overview(user_request, key_terms, content_by_type)\n        elif section_type == 'background':\n            return self._generate_contextual_background(user_request, all_content, content_by_type)\n        elif section_type == 'requirements':\n            return self._generate_contextual_requirements(user_request, requirements, key_terms)\n        elif section_type == 'architecture':\n            return self._generate_contextual_architecture(user_request, key_terms, content_by_type)\n        elif section_type == 'implementation':\n            return self._generate_contextual_implementation(user_request, key_terms, requirements)\n        elif section_type == 'data_flow':\n            return self._generate_data_flow_section(user_request, key_terms)\n        elif section_type == 'security':\n            return self._generate_security_section(user_request, key_terms)\n        elif section_type == 'performance':\n            return self._generate_performance_section(user_request, requirements)\n        elif section_type == 'testing':\n            return self._generate_testing_section(user_request, requirements)\n        elif section_type == 'deployment':\n            return self._generate_deployment_section(user_request, key_terms)\n        elif section_type == 'timeline':\n            return self._generate_timeline_section(user_request, requirements)\n        elif section_type == 'risks':\n            return self._generate_risks_section(user_request, key_terms)\n        else:\n            return f\"Content for {section_type} will be developed based on detailed analysis.\"\n\n    def _generate_contextual_overview(self, user_request, key_terms, content_by_type):\n        \"\"\"Generate overview using retrieved content context\"\"\"\n        content_types = list(content_by_type.keys())\n        \n        overview = f\"\"\"This technical design document outlines the comprehensive approach for implementing: **{user_request}**\n\n**Project Scope:**\nBased on analysis of {len(content_by_type)} different content sources ({', '.join(content_types)}), this solution addresses the following key areas:\n\n\"\"\"\n        \n        if key_terms:\n            overview += f\"\"\"**Key Technologies Identified from Documentation:**\n{', '.join(key_terms[:8])}\n\n\"\"\"\n        \n        overview += f\"\"\"**Solution Approach:**\nThe design incorporates insights from existing organizational documentation and leverages identified best practices to ensure:\n- Alignment with current technical standards and patterns\n- Integration with existing systems and workflows\n- Scalable and maintainable architecture based on proven approaches\n- Security and compliance requirements derived from organizational standards\n\n**Documentation Analysis:**\nThis design is informed by {sum(len(docs) for docs in content_by_type.values())} relevant documents from your knowledge base, ensuring contextual relevance and organizational alignment.\"\"\"\n\n        return overview\n\n    def _generate_contextual_background(self, user_request, all_content, content_by_type):\n        \"\"\"Generate background section with context from retrieved documents\"\"\"\n        \n        background = f\"\"\"**Current State Analysis:**\nThe need for {user_request} has been identified through comprehensive analysis of existing documentation and organizational requirements.\n\n**Context from Available Documentation:**\n\"\"\"\n        \n        # Summarize content by type with actual insights\n        for content_type, docs in content_by_type.items():\n            if docs:\n                # Get the highest scoring document for this type\n                top_doc = max(docs, key=lambda x: x['score'])\n                background += f\"\"\"\n**{content_type.title()} Sources ({len(docs)} documents):**\n- Primary insight: {top_doc['content'][:200]}...\n- Relevance score: {top_doc['score']:.3f}\n\"\"\"\n        \n        background += f\"\"\"\n**Problem Statement:**\nBased on the analyzed documentation, this design addresses the implementation of {user_request} while ensuring:\n- Compatibility with existing systems and documented patterns\n- Adherence to established organizational practices and standards\n- Meeting identified business and technical requirements from multiple sources\n- Leveraging existing knowledge and avoiding reinvention\n\n**Stakeholder Requirements:**\nThe solution incorporates requirements and insights identified across {len(content_by_type)} different content types, ensuring comprehensive coverage of both functional and operational needs.\"\"\"\n\n        return background\n\n    def _generate_contextual_requirements(self, user_request, requirements, key_terms):\n        \"\"\"Generate requirements based on extracted content\"\"\"\n        req_section = f\"\"\"**Functional Requirements:**\nBased on analysis of available documentation, the following functional requirements have been identified:\n\n\"\"\"\n        \n        if requirements['functional']:\n            for i, req in enumerate(requirements['functional'][:5], 1):\n                req_section += f\"{i}. {req}\\n\"\n        else:\n            req_section += f\"\"\"1. Core {user_request} functionality implementation\n2. User interface and interaction requirements based on organizational standards\n3. Data processing and management capabilities\n4. Integration with existing systems and documented APIs\n5. Reporting and monitoring features aligned with current practices\n\"\"\"\n        \n        req_section += f\"\"\"\n**Non-Functional Requirements:**\n\"\"\"\n        \n        if requirements['non_functional']:\n            for req in requirements['non_functional'][:5]:\n                req_section += f\"- {req}\\n\"\n        else:\n            req_section += \"\"\"- Performance: Response time < 2 seconds for standard operations\n- Scalability: Support for concurrent users and growing data volumes\n- Availability: 99.9% uptime with minimal planned downtime\n- Security: Industry-standard encryption and authentication\n- Maintainability: Well-documented, modular code architecture\n- Compliance: Adherence to organizational security and data policies\n\"\"\"\n        \n        if key_terms:\n            req_section += f\"\"\"\n**Technology Requirements (from documentation analysis):**\n- Integration with identified technologies: {', '.join(key_terms[:5])}\n- Compatibility with existing technology stack\n\"\"\"\n        \n        return req_section\n\n    def _generate_contextual_architecture(self, user_request, key_terms, content_by_type):\n        \"\"\"Generate architecture section based on retrieved content\"\"\"\n        \n        arch_section = f\"\"\"**System Architecture Overview:**\nThe {user_request} solution follows a modern, scalable architecture designed to integrate with existing organizational systems and patterns.\n\n**Core Components:**\n1. **Presentation Layer**\n   - User interface components following organizational design standards\n   - API gateway and routing based on existing patterns\n   - Authentication and session management integration\n\n2. **Business Logic Layer**\n   - Core {user_request} processing aligned with business rules\n   - Workflow orchestration following documented processes\n   - Service integration with existing business systems\n\n3. **Data Layer**\n   - Primary data storage using organizational standards\n   - Caching mechanisms based on performance requirements\n   - Data access patterns consistent with existing systems\n\n4. **Integration Layer**\n   - External system connectors for documented integrations\n   - Message queuing and processing using established patterns\n   - Event handling aligned with organizational event architecture\n\n\"\"\"\n        \n        if key_terms:\n            arch_section += f\"\"\"**Technology Stack (based on documentation analysis):**\n- Identified technologies: {', '.join(key_terms[:6])}\n- Architecture patterns: Microservices, API-first, Event-driven\n- Integration approaches: RESTful APIs, Message queues, Event streaming\n\"\"\"\n        \n        if content_by_type:\n            arch_section += f\"\"\"\n**Integration Context:**\nBased on analysis of {len(content_by_type)} content types, the architecture ensures:\n- Compatibility with existing {', '.join(content_by_type.keys())} systems\n- Adherence to documented architectural patterns and standards\n- Seamless integration with current technology ecosystem\n\"\"\"\n        \n        return arch_section\n\n    def _generate_contextual_implementation(self, user_request, key_terms, requirements):\n        \"\"\"Generate implementation section with retrieved content context\"\"\"\n        \n        impl_section = f\"\"\"**Implementation Strategy:**\nThe development of {user_request} will follow an iterative, risk-driven approach based on organizational best practices.\n\n**Development Phases:**\n\n**Phase 1: Foundation (Weeks 1-3)**\n- Core infrastructure setup using identified technologies\n- Basic {user_request} functionality implementation\n- Database schema design based on documented data models\n- Authentication framework integration with existing systems\n\n**Phase 2: Core Features (Weeks 4-6)**\n- Primary business logic implementation following documented patterns\n- User interface development aligned with organizational standards\n- API development using established conventions\n- Integration with key systems identified in documentation\n\n**Phase 3: Advanced Features (Weeks 7-8)**\n- Advanced functionality based on extracted requirements\n- Performance optimization using documented best practices\n- Security implementation following organizational standards\n- Comprehensive testing aligned with quality processes\n\n**Phase 4: Deployment (Weeks 9-10)**\n- Production environment setup using established patterns\n- Deployment automation following organizational DevOps practices\n- Monitoring and alerting integration with existing systems\n- Documentation and training based on organizational standards\n\n\"\"\"\n        \n        if key_terms:\n            impl_section += f\"\"\"**Technology Implementation:**\nBased on documentation analysis, implementation will leverage:\n- Core technologies: {', '.join(key_terms[:4])}\n- Development patterns: Following documented organizational standards\n- Integration approaches: Using established APIs and protocols\n\"\"\"\n        \n        impl_section += f\"\"\"\n**Development Standards:**\n- Code review processes aligned with organizational practices\n- Automated testing following documented quality standards\n- Continuous integration using established CI/CD pipelines\n- Documentation standards consistent with organizational requirements\n- Security practices based on documented security policies\n\"\"\"\n        \n        return impl_section\n\n    # Add placeholder methods for other sections\n    def _generate_data_flow_section(self, user_request, key_terms):\n        return f\"\"\"**Data Flow Architecture:**\nThe {user_request} system processes data through documented organizational patterns:\n\n**Input Processing:**\n- Data ingestion following established data pipeline patterns\n- Validation using organizational data quality standards\n- Transformation based on documented data models\n\n**Core Processing:**\n- Business logic execution aligned with documented processes\n- State management using established patterns\n- Event generation following organizational event architecture\n\n**Output Generation:**\n- Result formatting based on organizational standards\n- Integration with existing reporting systems\n- API responses following documented conventions\n\n**Technology Integration:**\n{f\"- Leveraging identified technologies: {', '.join(key_terms[:4])}\" if key_terms else \"- Using organizational standard technology stack\"}\n- Following documented data architecture patterns\n- Ensuring compliance with data governance policies\n\"\"\"\n\n    def _generate_security_section(self, user_request, key_terms):\n        return f\"\"\"**Security Framework:**\nThe {user_request} implementation incorporates comprehensive security measures based on organizational standards:\n\n**Authentication & Authorization:**\n- Integration with existing identity management systems\n- Role-based access control following organizational patterns\n- Multi-factor authentication using established protocols\n- Session management aligned with security policies\n\n**Data Protection:**\n- Encryption standards based on organizational requirements\n- Data classification following documented policies\n- Access controls aligned with data governance standards\n- Audit logging using established security monitoring\n\n**Application Security:**\n- Security testing following organizational security practices\n- Vulnerability management using established processes\n- Code security reviews aligned with development standards\n- Compliance with documented security policies\n\n**Technology Security:**\n{f\"- Security implementation for identified technologies: {', '.join(key_terms[:3])}\" if key_terms else \"- Following organizational technology security standards\"}\n- Integration with existing security infrastructure\n- Monitoring and alerting using established security tools\n\"\"\"\n\n    def _generate_performance_section(self, user_request, requirements):\n        return f\"\"\"**Performance Requirements:**\nThe {user_request} system is designed for optimal performance based on organizational standards:\n\n**Response Time Targets:**\n- API responses: Following organizational SLA requirements\n- User interface: Based on documented user experience standards\n- Batch processing: Aligned with existing system performance expectations\n\n**Scalability Design:**\n- Horizontal scaling using established infrastructure patterns\n- Load balancing following organizational deployment standards\n- Auto-scaling based on documented capacity planning approaches\n\n**Performance Optimization:**\n- Caching strategies using organizational standard technologies\n- Database optimization following documented best practices\n- Monitoring integration with existing performance management systems\n\n**Performance Testing:**\n- Load testing using established testing frameworks\n- Performance benchmarking against organizational standards\n- Capacity planning following documented processes\n\"\"\"\n\n    def _generate_testing_section(self, user_request, requirements):\n        return f\"\"\"**Testing Strategy:**\nComprehensive testing approach for {user_request} following organizational quality standards:\n\n**Testing Framework:**\n- Unit testing using established organizational frameworks\n- Integration testing following documented testing patterns\n- End-to-end testing aligned with quality assurance processes\n- Performance testing using organizational standard tools\n\n**Quality Assurance:**\n- Code review processes following organizational standards\n- Automated testing integration with existing CI/CD pipelines\n- Test coverage requirements based on organizational policies\n- Quality gates aligned with documented quality standards\n\n**Test Automation:**\n- Automated test execution using established testing infrastructure\n- Regression testing following organizational testing practices\n- Test reporting integration with existing quality management systems\n\"\"\"\n\n    def _generate_deployment_section(self, user_request, key_terms):\n        return f\"\"\"**Deployment Strategy:**\nThe {user_request} system deployment follows organizational DevOps practices:\n\n**Deployment Pipeline:**\n- CI/CD integration with existing organizational pipelines\n- Environment management following established patterns\n- Deployment automation using organizational standard tools\n- Release management aligned with documented processes\n\n**Infrastructure:**\n- Deployment using organizational standard infrastructure\n- Monitoring integration with existing operational systems\n- Backup and recovery following documented procedures\n- Security compliance with organizational deployment standards\n\n**Technology Deployment:**\n{f\"- Deployment of identified technologies: {', '.join(key_terms[:3])}\" if key_terms else \"- Using organizational standard deployment technologies\"}\n- Configuration management following established practices\n- Environment consistency using documented deployment patterns\n\"\"\"\n\n    def _generate_timeline_section(self, user_request, requirements):\n        return f\"\"\"**Project Timeline:**\nEstimated timeline for {user_request} implementation based on organizational project management standards:\n\n**Phase-based Timeline:**\n- **Weeks 1-3**: Foundation and setup following organizational onboarding processes\n- **Weeks 4-6**: Core development using established development practices\n- **Weeks 7-8**: Integration and testing following organizational quality processes\n- **Weeks 9-10**: Deployment using established deployment procedures\n\n**Key Milestones:**\n- Technical design approval: Following organizational review processes\n- Development milestones: Aligned with organizational project management standards\n- Testing completion: Based on documented quality gates\n- Production deployment: Following organizational go-live procedures\n\n**Risk Management:**\n- Timeline risks managed using organizational project management practices\n- Resource allocation following established resource management processes\n- Dependency management aligned with organizational project coordination\n\"\"\"\n\n    def _generate_risks_section(self, user_request, key_terms):\n        return f\"\"\"**Risk Assessment:**\nRisk identification and mitigation for {user_request} based on organizational risk management practices:\n\n**Technical Risks:**\n- Integration complexity with existing systems\n- Performance risks based on documented system constraints\n- Security risks managed through organizational security practices\n- Technology risks for identified technologies: {', '.join(key_terms[:3]) if key_terms else 'standard technology stack'}\n\n**Project Risks:**\n- Resource availability managed through organizational resource planning\n- Timeline risks mitigated using established project management practices\n- Scope management following organizational change control processes\n\n**Mitigation Strategies:**\n- Risk monitoring using organizational risk management tools\n- Escalation procedures following documented organizational processes\n- Contingency planning based on organizational risk management standards\n- Regular risk reviews aligned with project management practices\n\"\"\"\n\n    def generate_design_document(self, user_request, title=None):\n        \"\"\"Generate enhanced design document using RAG content analysis\"\"\"\n        start_time = datetime.now()\n        \n        # Generate title\n        if not title:\n            title = f\"Technical Design Document: {user_request}\"\n        \n        print(f\"🔍 Analyzing relevant content for: {user_request}\")\n        \n        # Extract relevant content from documents using RAG\n        sources, content_by_type, all_content = self._extract_relevant_content(user_request, k=12)\n        \n        print(f\"📚 Found {len(sources)} relevant sources across {len(content_by_type)} content types\")\n        \n        # Generate sections using retrieved content context\n        sections = {}\n        section_types = [\n            'overview', 'background', 'requirements', 'architecture', \n            'implementation', 'data_flow', 'security', 'performance', \n            'testing', 'deployment', 'timeline', 'risks'\n        ]\n        \n        print(\"🤖 Generating contextual content sections...\")\n        for section_type in section_types:\n            try:\n                sections[section_type] = self._analyze_content_for_section(\n                    section_type, user_request, content_by_type, all_content\n                )\n            except Exception as e:\n                print(f\"⚠️ Error generating {section_type}: {e}\")\n                sections[section_type] = f\"Content for {section_type} section will be developed based on detailed analysis.\"\n        \n        # Format references with enhanced information\n        references = self._format_enhanced_references(sources)\n        content_types_str = ', '.join(content_by_type.keys()) if content_by_type else 'Various'\n        \n        # Template variables\n        template_vars = {\n            'title': title,\n            'overview': sections['overview'],\n            'background': sections['background'],\n            'requirements': sections['requirements'],\n            'architecture': sections['architecture'],\n            'implementation': sections['implementation'],\n            'data_flow': sections['data_flow'],\n            'security': sections['security'],\n            'performance': sections['performance'],\n            'testing': sections['testing'],\n            'deployment': sections['deployment'],\n            'timeline': sections['timeline'],\n            'risks': sections['risks'],\n            'references': references,\n            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'source_count': len(sources),\n            'content_types': content_types_str\n        }\n        \n        # Generate document\n        template = Template(self.design_doc_template)\n        design_document = template.render(**template_vars)\n        \n        generation_time = (datetime.now() - start_time).total_seconds()\n        \n        print(f\"✅ Enhanced document generated in {generation_time:.2f}s using {len(sources)} sources\")\n        \n        return {\n            'document': design_document,\n            'sources': sources,\n            'content_analysis': {\n                'content_by_type': content_by_type,\n                'total_content_length': len(all_content),\n                'key_insights': f\"Analyzed {len(sources)} documents across {len(content_by_type)} content types\"\n            },\n            'metadata': {\n                'title': title,\n                'user_request': user_request,\n                'timestamp': template_vars['timestamp'],\n                'generation_time': f\"{generation_time:.2f}s\",\n                'source_count': len(sources),\n                'content_types': list(content_by_type.keys()),\n                'rag_enhanced': True\n            }\n        }\n\n    def _format_enhanced_references(self, sources):\n        \"\"\"Format references with detailed information from RAG analysis\"\"\"\n        if not sources:\n            return \"No references available from the knowledge base.\"\n        \n        references = []\n        \n        # Group by content type for better organization\n        by_type = defaultdict(list)\n        for source in sources:\n            by_type[source['type']].append(source)\n        \n        for content_type, type_sources in by_type.items():\n            references.append(f\"\\n**{content_type.title()} Sources:**\")\n            for i, source in enumerate(type_sources, 1):\n                ref = f\"{i}. **{source['title']}**\"\n                if source.get('url'):\n                    ref += f\" - [Link]({source['url']})\"\n                if source.get('page'):\n                    ref += f\" (Page {source['page']})\"\n                ref += f\" (Relevance: {source['score']:.3f})\"\n                \n                # Add content preview for context\n                if source.get('content_preview'):\n                    ref += f\"\\n   Preview: {source['content_preview']}\"\n                \n                references.append(ref)\n        \n        return '\\n'.join(references)\n\n    def save_design_document(self, document_data, filename=None):\n        \"\"\"Fast document saving\"\"\"\n        if not filename:\n            safe_title = ''.join(c for c in document_data['metadata']['title'] if c.isalnum() or c in (' ', '-', '_'))\n            safe_title = safe_title.replace(' ', '_')[:30]  # Shorter filename\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            filename = f'{safe_title}_{timestamp}.md'\n        \n        try:\n            with open(filename, 'w', encoding='utf-8') as f:\n                f.write(document_data['document'])\n            print(f'✅ Document saved: {filename}')\n            return filename\n        except Exception as e:\n            print(f'❌ Save error: {e}')\n            return None\n\n# Test function\ndef test_enhanced_generator():\n    \"\"\"Test the enhanced RAG-powered generator\"\"\"\n    print(\"🧪 Testing Enhanced RAG Design Document Generator...\")\n    \n    try:\n        generator = DesignDocumentGenerator()\n        \n        # Test with a comprehensive request\n        result = generator.generate_design_document(\n            \"User Authentication and Authorization System with OAuth2 Integration and Multi-Factor Authentication\"\n        )\n        \n        print(f\"✅ Generated document with {result['metadata']['source_count']} sources\")\n        print(f\"📄 Document length: {len(result['document'])} characters\")\n        print(f\"⏱️ Generation time: {result['metadata']['generation_time']}\")\n        print(f\"🔍 Content types analyzed: {', '.join(result['metadata']['content_types'])}\")\n        print(f\"🤖 RAG enhanced: {result['metadata']['rag_enhanced']}\")\n        \n        # Show content analysis summary\n        if 'content_analysis' in result:\n            print(f\"📊 Content analysis: {result['content_analysis']['key_insights']}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"❌ Test failed: {e}\")\n        traceback.print_exc()\n        return False\n\nif __name__ == \"__main__\":\n    test_enhanced_generator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:22:49.612900Z","iopub.execute_input":"2025-07-27T05:22:49.613239Z","iopub.status.idle":"2025-07-27T05:22:49.638341Z","shell.execute_reply.started":"2025-07-27T05:22:49.613214Z","shell.execute_reply":"2025-07-27T05:22:49.637279Z"}},"outputs":[{"name":"stdout","text":"Writing design_doc_generator.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### 4. Streamlit Frontend Application","metadata":{}},{"cell_type":"code","source":"%%writefile streamlit_app.py\nimport streamlit as st\nimport os\nimport sys\nimport tempfile\nimport traceback\nfrom io import BytesIO\nimport pandas as pd\nfrom datetime import datetime\nimport json\n\n# Import our modules with fallback handling\ntry:\n    from unified_processor import UnifiedDataProcessor\nexcept ImportError:\n    st.error(\"Could not import UnifiedDataProcessor. Please check your files.\")\n    st.stop()\n\ntry:\n    from design_doc_generator import DesignDocumentGenerator\nexcept ImportError:\n    st.error(\"Could not import DesignDocumentGenerator. Please check your files.\")\n    st.stop()\n\ntry:\n    from extract_confluence import ConfluenceProcessor\nexcept ImportError:\n    st.error(\"Could not import ConfluenceProcessor. Please check your files.\")\n    st.stop()\n\nst.set_page_config(\n    page_title='AI Document Assistant',\n    page_icon='📚',\n    layout='wide',\n    initial_sidebar_state='expanded'\n)\n\n@st.cache_resource\ndef initialize_processor():\n    return UnifiedDataProcessor()\n\n@st.cache_resource\ndef initialize_confluence():\n    return ConfluenceProcessor()\n\ndef initialize_doc_generator():\n    try:\n        return DesignDocumentGenerator()\n    except Exception as e:\n        st.session_state.doc_generator_error = str(e)\n        return None\n\n# Initialize components\nif 'processor' not in st.session_state:\n    st.session_state.processor = initialize_processor()\n\nif 'confluence_processor' not in st.session_state:\n    st.session_state.confluence_processor = initialize_confluence()\n\n# Header\nst.title('📚 AI Document Assistant')\nst.markdown('**RAG System with Enhanced PDF and Confluence Integration**')\n\n# Sidebar - System Status\nwith st.sidebar:\n    st.title('🔧 System Status')\n    \n    # Vector store status\n    vectorstore = st.session_state.processor.get_vectorstore()\n    has_documents = vectorstore is not None\n    \n    if has_documents:\n        try:\n            doc_count = vectorstore.index.ntotal if hasattr(vectorstore, 'index') else 'unknown'\n            st.success(f'✅ Vector Store: {doc_count} vectors')\n        except:\n            st.success('✅ Vector Store: Active')\n    else:\n        st.warning('⚠️ No documents loaded')\n    \n    # Confluence status\n    confluence_status = st.session_state.processor.get_confluence_status()\n    if confluence_status['available']:\n        st.success(f'✅ Confluence: Connected')\n    else:\n        st.error(f'❌ Confluence: {confluence_status[\"error\"]}')\n\n# Main content tabs\ntab1, tab2, tab3, tab4, tab5 = st.tabs(['🔍 Query Documents', '📄 Upload PDFs', '🌐 Confluence Integration', '📋 Generate Design Doc', '🧪 System Test'])\n\n# Query Documents tab\nwith tab1:\n    st.header('🔍 Query Your Documents')\n    \n    if not has_documents:\n        st.warning('⚠️ No documents available. Please upload PDFs or ingest Confluence data first.')\n    else:\n        col1, col2 = st.columns([3, 1])\n        with col1:\n            query = st.text_input('Enter your question:', placeholder='What would you like to know?')\n        with col2:\n            k_results = st.slider('Results:', 1, 10, 5)\n        \n        if st.button('🔍 Search', type='primary', use_container_width=True):\n            if query:\n                with st.spinner('🔍 Searching documents...'):\n                    try:\n                        results = st.session_state.processor.search_documents(query, k=k_results)\n                        \n                        if results:\n                            st.success(f'✅ Found {len(results)} relevant results')\n                            \n                            for i, (doc, score) in enumerate(results, 1):\n                                with st.expander(f'📄 Result {i} - Relevance Score: {score:.4f}'):\n                                    col1, col2 = st.columns([1, 1])\n                                    \n                                    with col1:\n                                        st.markdown(f'**Source:** {doc.metadata.get(\"source\", \"unknown\")}')\n                                        st.markdown(f'**Title:** {doc.metadata.get(\"title\", \"Unknown\")}')\n                                        if doc.metadata.get('page'):\n                                            st.markdown(f'**Page:** {doc.metadata.get(\"page\")}')\n                                    \n                                    with col2:\n                                        if doc.metadata.get('url'):\n                                            st.markdown(f'**URL:** [Link]({doc.metadata.get(\"url\")})')\n                                        if doc.metadata.get('file_name'):\n                                            st.markdown(f'**File:** {doc.metadata.get(\"file_name\")}')\n                                    \n                                    st.markdown('**Content:**')\n                                    # Fixed: Added proper label and label_visibility\n                                    st.text_area(\n                                        'Document Content', \n                                        doc.page_content, \n                                        height=150, \n                                        key=f'content_{i}',\n                                        label_visibility='collapsed'\n                                    )\n                        else:\n                            st.warning('❌ No relevant results found')\n                    except Exception as e:\n                        st.error(f'❌ Search error: {str(e)}')\n                        with st.expander('Error Details'):\n                            st.code(traceback.format_exc())\n            else:\n                st.warning('⚠️ Please enter a search query')\n\n# Upload PDFs tab\nwith tab2:\n    st.header('📄 Upload PDF Documents')\n    \n    uploaded_files = st.file_uploader(\n        'Choose PDF files',\n        type=['pdf'],\n        accept_multiple_files=True,\n        help='Upload one or more PDF files to add to the knowledge base'\n    )\n    \n    col1, col2 = st.columns([1, 1])\n    with col1:\n        chunk_size = st.slider('Chunk Size:', 200, 1000, 500, step=50)\n    with col2:\n        st.info(f'Recommended: 500 for balanced performance')\n    \n    if uploaded_files:\n        st.write(f'📁 Selected {len(uploaded_files)} file(s):')\n        for file in uploaded_files:\n            st.write(f'• {file.name} ({file.size:,} bytes)')\n        \n        if st.button('📤 Process PDFs', type='primary', use_container_width=True):\n            progress_bar = st.progress(0)\n            status_text = st.empty()\n            \n            try:\n                temp_files = []\n                for i, uploaded_file in enumerate(uploaded_files):\n                    status_text.text(f'Processing {uploaded_file.name}...')\n                    progress_bar.progress((i + 1) / len(uploaded_files))\n                    \n                    # Save to temporary file\n                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n                        tmp_file.write(uploaded_file.getvalue())\n                        temp_files.append(tmp_file.name)\n                \n                # Process all files\n                status_text.text('Adding to vector store...')\n                st.session_state.processor.add_pdf_documents(temp_files, chunk_size=chunk_size)\n                \n                # Cleanup\n                for temp_file in temp_files:\n                    try:\n                        os.unlink(temp_file)\n                    except:\n                        pass\n                \n                progress_bar.progress(1.0)\n                status_text.text('✅ Processing complete!')\n                st.success(f'✅ Successfully processed {len(uploaded_files)} PDF file(s)')\n                st.rerun()\n                \n            except Exception as e:\n                st.error(f'❌ Error processing PDFs: {str(e)}')\n                with st.expander('Error Details'):\n                    st.code(traceback.format_exc())\n\n# Confluence Integration tab\nwith tab3:\n    st.header('🌐 Confluence Integration')\n    \n    # Show connection status\n    confluence_status = st.session_state.processor.get_confluence_status()\n    \n    col1, col2 = st.columns([1, 1])\n    with col1:\n        st.subheader('Connection Status')\n        if confluence_status['available']:\n            st.success('✅ Connected to Confluence')\n            st.info(f\"URL: {confluence_status['url']}\")\n            st.info(f\"User: {confluence_status['username']}\")\n        else:\n            st.error('❌ Confluence not available')\n            st.error(f\"Error: {confluence_status['error']}\")\n    \n    with col2:\n        st.subheader('Configuration')\n        st.info('Set these environment variables:')\n        st.code('''\nCONFLUENCE_URL=https://your-domain.atlassian.net\nCONFLUENCE_USERNAME=your-email@domain.com\nCONFLUENCE_API_TOKEN=your-api-token\n        ''')\n    \n    if confluence_status['available']:\n        st.subheader('📄 Ingest Confluence Pages')\n        \n        page_ids_input = st.text_area(\n            'Page IDs (one per line):',\n            placeholder='123456789\\n987654321\\n...',\n            help='Enter Confluence page IDs, one per line'\n        )\n        \n        col1, col2 = st.columns([1, 1])\n        with col1:\n            chunk_size = st.slider('Chunk Size:', 200, 1000, 500, step=50, key='confluence_chunk')\n        with col2:\n            st.info('Smaller chunks = more precise search')\n        \n        if st.button('📥 Ingest Pages', type='primary', use_container_width=True):\n            if page_ids_input.strip():\n                page_ids = [pid.strip() for pid in page_ids_input.strip().split('\\n') if pid.strip()]\n                \n                with st.spinner(f'🔄 Processing {len(page_ids)} Confluence page(s)...'):\n                    try:\n                        st.session_state.processor.add_confluence_documents(\n                            page_ids=page_ids,\n                            chunk_size=chunk_size\n                        )\n                        st.success(f'✅ Successfully processed {len(page_ids)} Confluence page(s)')\n                        st.rerun()\n                    except Exception as e:\n                        st.error(f'❌ Error processing Confluence pages: {str(e)}')\n                        with st.expander('Error Details'):\n                            st.code(traceback.format_exc())\n            else:\n                st.warning('⚠️ Please enter at least one page ID')\n\n# Generate Design Doc tab\nwith tab4:\n    st.header('📋 Generate Design Document')\n    \n    if not has_documents:\n        st.warning('⚠️ No documents available. Upload PDFs or ingest Confluence data first for better results.')\n    \n    col1, col2 = st.columns([2, 1])\n    with col1:\n        user_request = st.text_area(\n            'Describe what you want to build:',\n            placeholder='e.g., A microservice for user authentication with OAuth2 integration...',\n            height=100\n        )\n    with col2:\n        doc_title = st.text_input(\n            'Document Title (optional):',\n            placeholder='Auto-generated if empty'\n        )\n    \n    if st.button('📋 Generate Design Document', type='primary', use_container_width=True):\n        if user_request.strip():\n            with st.spinner('🤖 Generating design document...'):\n                try:\n                    # Initialize doc generator\n                    doc_generator = initialize_doc_generator()\n                    \n                    if doc_generator:\n                        result = doc_generator.generate_design_document(\n                            user_request=user_request.strip(),\n                            title=doc_title.strip() if doc_title.strip() else None\n                        )\n                        \n                        st.success('✅ Design document generated successfully!')\n                        \n                        # Display metadata\n                        col1, col2, col3 = st.columns([1, 1, 1])\n                        with col1:\n                            st.metric('Sources Used', result['metadata']['source_count'])\n                        with col2:\n                            st.metric('LLM Type', result['metadata']['llm_used'])\n                        with col3:\n                            st.metric('Generated', result['metadata']['timestamp'])\n                        \n                        # Display document\n                        st.subheader('📄 Generated Document')\n                        st.markdown(result['document'])\n                        \n                        # Download button\n                        st.download_button(\n                            label='💾 Download Document',\n                            data=result['document'],\n                            file_name=f\"design_doc_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\",\n                            mime='text/markdown',\n                            use_container_width=True\n                        )\n                        \n                        # Show sources\n                        if result['sources']:\n                            with st.expander('📚 Sources Used'):\n                                for i, source in enumerate(result['sources'], 1):\n                                    st.write(f\"{i}. **{source['title']}** ({source['type']}) - Score: {source['score']:.3f}\")\n                                    if source.get('url'):\n                                        st.write(f\"   🔗 [Link]({source['url']})\")\n                    else:\n                        st.error('❌ Failed to initialize document generator')\n                        if hasattr(st.session_state, 'doc_generator_error'):\n                            st.error(f\"Error: {st.session_state.doc_generator_error}\")\n                \n                except Exception as e:\n                    st.error(f'❌ Error generating document: {str(e)}')\n                    with st.expander('Error Details'):\n                        st.code(traceback.format_exc())\n        else:\n            st.warning('⚠️ Please describe what you want to build')\n\n# System Test tab\nwith tab5:\n    st.header('🧪 System Test & Diagnostics')\n    \n    col1, col2 = st.columns([1, 1])\n    \n    with col1:\n        st.subheader('🔍 Component Status')\n        \n        # Test vector store\n        if st.button('Test Vector Store', use_container_width=True):\n            try:\n                vectorstore = st.session_state.processor.get_vectorstore()\n                if vectorstore:\n                    # Try a simple search\n                    test_results = st.session_state.processor.search_documents('test', k=1)\n                    st.success(f'✅ Vector store working - {len(test_results)} results')\n                else:\n                    st.warning('⚠️ No vector store available')\n            except Exception as e:\n                st.error(f'❌ Vector store error: {str(e)}')\n        \n        # Test Confluence\n        if st.button('Test Confluence', use_container_width=True):\n            try:\n                status = st.session_state.processor.get_confluence_status()\n                if status['available']:\n                    st.success('✅ Confluence connection working')\n                else:\n                    st.error(f'❌ Confluence error: {status[\"error\"]}')\n            except Exception as e:\n                st.error(f'❌ Confluence test error: {str(e)}')\n        \n        # Test Document Generator\n        if st.button('Test Document Generator', use_container_width=True):\n            try:\n                doc_gen = initialize_doc_generator()\n                if doc_gen:\n                    st.success('✅ Document generator initialized')\n                else:\n                    st.error('❌ Document generator failed to initialize')\n            except Exception as e:\n                st.error(f'❌ Document generator error: {str(e)}')\n    \n    with col2:\n        st.subheader('📊 System Information')\n        \n        # Environment variables\n        env_vars = ['CONFLUENCE_URL', 'CONFLUENCE_USERNAME', 'CONFLUENCE_API_TOKEN']\n        st.write('**Environment Variables:**')\n        for var in env_vars:\n            value = os.getenv(var, 'Not set')\n            if var == 'CONFLUENCE_API_TOKEN' and value != 'Not set':\n                value = f'{value[:8]}...' if len(value) > 8 else '***'\n            st.write(f'• {var}: {value}')\n        \n        # File system\n        st.write('**File System:**')\n        vector_store_path = './vector_store/'\n        if os.path.exists(vector_store_path):\n            files = os.listdir(vector_store_path)\n            st.write(f'• Vector store files: {len(files)}')\n            for file in files[:5]:  # Show first 5 files\n                st.write(f'  - {file}')\n        else:\n            st.write('• Vector store directory: Not found')\n    \n    # Clear data section\n    st.subheader('🗑️ Data Management')\n    col1, col2 = st.columns([1, 1])\n    \n    with col1:\n        if st.button('🗑️ Clear Vector Store', type='secondary', use_container_width=True):\n            try:\n                vector_store_path = './vector_store/'\n                if os.path.exists(vector_store_path):\n                    import shutil\n                    shutil.rmtree(vector_store_path)\n                    st.success('✅ Vector store cleared')\n                    st.rerun()\n                else:\n                    st.info('ℹ️ No vector store to clear')\n            except Exception as e:\n                st.error(f'❌ Error clearing vector store: {str(e)}')\n    \n    with col2:\n        if st.button('🔄 Restart Components', type='secondary', use_container_width=True):\n            try:\n                # Clear cached resources\n                st.cache_resource.clear()\n                # Reset session state\n                for key in ['processor', 'confluence_processor']:\n                    if key in st.session_state:\n                        del st.session_state[key]\n                st.success('✅ Components restarted')\n                st.rerun()\n            except Exception as e:\n                st.error(f'❌ Error restarting: {str(e)}')\n\n# Footer\nst.markdown('---')\nst.markdown('**AI Document Assistant** - Enhanced RAG system with PDF and Confluence integration')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:23:36.836708Z","iopub.execute_input":"2025-07-27T05:23:36.837190Z","iopub.status.idle":"2025-07-27T05:23:36.858090Z","shell.execute_reply.started":"2025-07-27T05:23:36.837164Z","shell.execute_reply":"2025-07-27T05:23:36.856659Z"}},"outputs":[{"name":"stdout","text":"Writing streamlit_app.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### 5. Environment Configuration","metadata":{}},{"cell_type":"code","source":"%%writefile .env\n# Confluence Configuration (Replace with your actual values)\nCONFLUENCE_URL=https://your-domain.atlassian.net/wiki\nCONFLUENCE_USERNAME=your-email@company.com\nCONFLUENCE_API_TOKEN=your-api-token\n\n# AWS Configuration for Bedrock (Optional - for LLM integration)\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nAWS_ACCESS_KEY_ID = user_secrets.get_secret(\"Access_key\")\nAWS_SECRET_ACCESS_KEY = user_secrets.get_secret(\"Secret_access_key\")\nAWS_DEFAULT_REGION=us-east-1\n\n#NGrok Auth token for the public access with url\nNGROK_AUTH_TOKEN = user_secrets.get_secret(\"ngrok_auth_token\")\n\n# HuggingFace Cache Directory\nHF_HOME=./huggingface_cache\n\n# Vector Store Configuration\nVECTOR_STORE_PATH=./vector_store/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:23:53.699504Z","iopub.execute_input":"2025-07-27T05:23:53.700375Z","iopub.status.idle":"2025-07-27T05:23:53.706785Z","shell.execute_reply.started":"2025-07-27T05:23:53.700331Z","shell.execute_reply":"2025-07-27T05:23:53.705601Z"}},"outputs":[{"name":"stdout","text":"Writing .env\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Test all components first\ndef comprehensive_test():\n    print('🧪 Comprehensive System Test')\n    print('=' * 50)\n    \n    success_count = 0\n    total_tests = 0\n    \n    # Test 1: Imports\n    total_tests += 1\n    try:\n        from unified_processor import UnifiedDataProcessor\n        from extract_confluence import ConfluenceProcessor\n        from design_doc_generator import DesignDocumentGenerator\n        print('✅ All imports successful')\n        success_count += 1\n    except Exception as e:\n        print(f'❌ Import error: {e}')\n        traceback.print_exc()\n    \n    # Test 2: Processor initialization\n    total_tests += 1\n    try:\n        processor = UnifiedDataProcessor()\n        print('✅ UnifiedDataProcessor initialized')\n        success_count += 1\n    except Exception as e:\n        print(f'❌ Processor error: {e}')\n        traceback.print_exc()\n        return\n    \n    # Test 3: Confluence validation\n    total_tests += 1\n    try:\n        confluence_status = processor.get_confluence_status()\n        if confluence_status['available']:\n            print(f'✅ Confluence connected: {confluence_status[url]}')\n            success_count += 1\n        else:\n            print(f'⚠️ Confluence not available: {confluence_status[error]}')\n    except Exception as e:\n        print(f'❌ Confluence test error: {e}')\n    \n    # Test 4: Search function\n    total_tests += 1\n    try:\n        results = processor.search_documents('test query', k=1)\n        print(f'✅ Search function works (found {len(results)} results)')\n        success_count += 1\n    except Exception as e:\n        print(f'❌ Search error: {e}')\n        traceback.print_exc()\n    \n    # Test 5: Design document generator\n    total_tests += 1\n    try:\n        doc_gen = DesignDocumentGenerator()\n        print('✅ Design Document Generator initialized')\n        success_count += 1\n    except Exception as e:\n        print(f'⚠️ Design generator (expected if no docs): {e}')\n    \n    print(f'\\\\n🎯 Test Results: {success_count}/{total_tests} tests passed')\n    if success_count >= 3:\n        print('🎉 Core system is ready!')\n    else:\n        print('⚠️ Some core tests failed. Check configuration.')\n    \n    return success_count >= 3\n\n# Run comprehensive test\ncomprehensive_test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:24:08.459469Z","iopub.execute_input":"2025-07-27T05:24:08.459834Z","iopub.status.idle":"2025-07-27T05:24:59.706071Z","shell.execute_reply.started":"2025-07-27T05:24:08.459802Z","shell.execute_reply":"2025-07-27T05:24:59.705233Z"}},"outputs":[{"name":"stdout","text":"🧪 Comprehensive System Test\n==================================================\n✅ All imports successful\n","output_type":"stream"},{"name":"stderr","text":"2025-07-27 05:24:29.436240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753593869.668116      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753593869.735367      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffb49a6d7b03467ebb1cd129227e9061"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4629b508945048a5b6d13cf55fd13a54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a76704638e410896dbc11f63828c50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c07f9a9f3c4da19d244296129473d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf4a2a8a5904188911ce49fc81b21c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d36ea879018d4a2baf5276ccba089493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967e3c24bbb14ec896f2e54f4d7d4345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f07c00f636f4e17af5b3372b4b88211"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bfb46b0d6ad4457bb5d029c789de344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31e385b3e6241ec8520112fe4ba311a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868515fb0ef0434eb9a30dcd4b95e749"}},"metadata":{}},{"name":"stdout","text":"Confluence connection error: Connection failed: Current user not permitted to use Confluence\nNo existing vector store found\n✅ UnifiedDataProcessor initialized\n❌ Confluence test error: name 'error' is not defined\nNo vector store available\n✅ Search function works (found 0 results)\n✅ Design Document Generator initialized\n\\n🎯 Test Results: 4/5 tests passed\n🎉 Core system is ready!\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 🚀 Running the Application\n\n### Method 1: Run Streamlit Directly","metadata":{}},{"cell_type":"code","source":"# Test the components first\nprint('🧪 Testing components...')\n\ntry:\n    from unified_processor import UnifiedDataProcessor\n    processor = UnifiedDataProcessor()\n    print('✅ Unified Processor: OK')\nexcept Exception as e:\n    print(f'❌ Unified Processor: {e}')\n\ntry:\n    from extract_confluence import ConfluenceProcessor\n    confluence = ConfluenceProcessor()\n    print(f'✅ Confluence Processor: {\"Available\" if confluence.available else \"Not configured\"}')\nexcept Exception as e:\n    print(f'❌ Confluence Processor: {e}')\n\nprint('\\n🎯 Components tested successfully!')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run Streamlit app\nimport subprocess\nimport threading\nimport time\n\ndef run_streamlit():\n    subprocess.run(['streamlit', 'run', 'streamlit_app.py', '--server.port=8502', '--server.headless=true'])\n\nprint('🚀 Starting Streamlit application...')\nstreamlit_thread = threading.Thread(target=run_streamlit)\nstreamlit_thread.daemon = True\nstreamlit_thread.start()\n\ntime.sleep(3)\nprint('✅ Streamlit should be running on port 8501')\nprint('🌐 If running locally, visit: http://localhost:8501')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Method 2: Using ngrok for Public Access (Kaggle)","metadata":{}},{"cell_type":"code","source":"# Method 2A: ngrok with Authentication (Public Access)\n# Set your ngrok authtoken here\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nNGROK_AUTH_TOKEN = user_secrets.get_secret(\"ngrok_auth_token\")\n\ndef setup_ngrok_with_auth(auth_token):\n    try:\n        from pyngrok import ngrok\n        import subprocess\n        import threading\n        import time\n        \n        # Set the authtoken\n        ngrok.set_auth_token(auth_token)\n        print('✅ ngrok authentication configured')\n        \n        # Kill any existing processes\n        subprocess.run(['pkill', '-f', 'streamlit'], capture_output=True)\n        \n        def run_streamlit_background():\n            subprocess.run([\n                'streamlit', 'run', 'streamlit_app.py',\n                '--server.port=8501',\n                '--server.headless=true',\n                '--server.enableCORS=false',\n                '--server.enableXsrfProtection=false'\n            ])\n        \n        print('🚀 Starting Streamlit...')\n        streamlit_thread = threading.Thread(target=run_streamlit_background)\n        streamlit_thread.daemon = True\n        streamlit_thread.start()\n        \n        # Wait for Streamlit to start\n        time.sleep(5)\n        \n        print('🌐 Creating public tunnel with ngrok...')\n        public_url = ngrok.connect(8501)\n        \n        print(f'\\n🎉 SUCCESS! Your app is now publicly accessible at:')\n        print(f'🔗 {public_url}')\n        print(f'\\n📱 Share this URL to access your AI Document Assistant!')\n        print(f'\\n⚡ Features available:')\n        print(f'   • Upload and process PDF documents')\n        print(f'   • Configure Confluence integration')\n        print(f'   • Query documents with semantic search')\n        print(f'   • Generate professional design documents')\n        \n        print(f'\\n⏳ Keeping the application running...')\n        print(f'💡 The tunnel will stay active as long as this cell is running')\n        \n        return public_url, ngrok\n        \n    except ImportError:\n        print('❌ pyngrok not available. Install with: !pip install pyngrok')\n        return None, None\n    except Exception as e:\n        print(f'❌ Error setting up ngrok: {e}')\n        print('💡 Make sure your authtoken is correct')\n        return None, None\n\n# Uncomment and set your token to use ngrok\npublic_url, ngrok_instance = setup_ngrok_with_auth(NGROK_AUTH_TOKEN)\nprint('💡 To use ngrok: Set your NGROK_AUTH_TOKEN above and uncomment the last line')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:25:27.830918Z","iopub.execute_input":"2025-07-27T05:25:27.831550Z","iopub.status.idle":"2025-07-27T05:25:34.357850Z","shell.execute_reply.started":"2025-07-27T05:25:27.831526Z","shell.execute_reply":"2025-07-27T05:25:34.356877Z"}},"outputs":[{"name":"stdout","text":"✅ ngrok authentication configured                                                                   \n🚀 Starting Streamlit...\n\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\n\n  You can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://172.19.2.2:8501\n  External URL: http://35.239.148.32:8501\n\n🌐 Creating public tunnel with ngrok...\n\n🎉 SUCCESS! Your app is now publicly accessible at:\n🔗 NgrokTunnel: \"https://7116a0b99a1b.ngrok-free.app\" -> \"http://localhost:8501\"\n\n📱 Share this URL to access your AI Document Assistant!\n\n⚡ Features available:\n   • Upload and process PDF documents\n   • Configure Confluence integration\n   • Query documents with semantic search\n   • Generate professional design documents\n\n⏳ Keeping the application running...\n💡 The tunnel will stay active as long as this cell is running\n💡 To use ngrok: Set your NGROK_AUTH_TOKEN above and uncomment the last line\n","output_type":"stream"},{"name":"stderr","text":"2025-07-27 05:26:53.582006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753594013.637028     169 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753594013.653779     169 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Confluence connection error: Connection failed: Current user not permitted to use Confluence\nNo existing vector store found\nConfluence connection error: Connection failed: Current user not permitted to use Confluence\nLoaded 13 pages from tmpuy9hxour.pdf\nSplit into 62 chunks.\nVector store saved to ./vector_store/\nAdded documents to unified vector store.\nSearching for: \"What is ai?\" (k=5)\nFound 5 results\nSearching for: \"What is collaborative ai?\" (k=1)\nFound 1 results\n🔍 Analyzing relevant content for: Write a small design document about Multiagent infra\nConfluence connection error: Connection failed: Current user not permitted to use Confluence\nLoaded existing vector store\n🔍 Searching for relevant content: Write a small design document about Multiagent infra\nSearching for: \"Write a small design document about Multiagent infra\" (k=12)\nFound 12 results\n📚 Found 12 relevant sources across 1 content types\n📚 Found 12 relevant sources across 1 content types\n🤖 Generating contextual content sections...\n✅ Enhanced document generated in 1.44s using 12 sources\n  Stopping...\n  Stopping...\n  Stopping...\n  Stopping...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 📖 Usage Instructions\n\n### 1. Upload PDF Documents\n- Go to the \"Upload PDFs\" tab\n- Select one or more PDF files\n- Adjust chunk size if needed (default: 500)\n- Click \"Upload and Process\"\n\n### 2. Configure Confluence (Optional)\n- Set environment variables:\n  - `CONFLUENCE_URL`: Your Confluence instance URL\n  - `CONFLUENCE_USERNAME`: Your email\n  - `CONFLUENCE_API_TOKEN`: API token from Atlassian\n\n### 3. Ingest Confluence Data\n- Go to \"Confluence Integration\" tab\n- Enter page IDs (comma-separated)\n- Click \"Ingest Confluence Data\"\n\n### 4. Query Documents\n- Go to \"Query Documents\" tab\n- Enter your question\n- Adjust number of results\n- Click \"Search\"\n\n### 5. Generate Design Documents\n- Go to \"Generate Design Doc\" tab\n- Describe what you want to design\n- Optionally provide a custom title\n- Click \"Generate Design Document\"\n- Download the generated document\n\n## 🎯 Features\n\n- ✅ PDF Processing: Upload and process multiple PDF documents\n- ✅ Confluence Integration: Ingest pages from Confluence\n- ✅ Vector Search: Semantic search across all documents\n- ✅ Design Document Generation: AI-powered design document creation\n- ✅ Web Interface: User-friendly Streamlit frontend\n- ✅ Real-time Status: System status monitoring\n- ✅ Download Support: Export generated documents","metadata":{}}]}